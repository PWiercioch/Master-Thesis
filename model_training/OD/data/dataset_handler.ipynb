{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set download location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "datasets_path = pathlib.Path().resolve() / \"raw_datasets\"\n",
    "fo.config.dataset_zoo_dir = datasets_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List avaialble datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bdd100k-train',\n",
       " 'bdd_train_6000',\n",
       " 'coco-2017-train-12000',\n",
       " 'kitti-train',\n",
       " 'kitti_train_6000']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Raw datasets already preapred \"Raw dataset preparation\" can be skipped - move to \"TF Record CReation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KITTI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train' already downloaded\n",
      "Loading existing dataset 'kitti-train'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "kitti_train = foz.load_zoo_dataset(\"kitti\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract 20% from training set for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = int(len(kitti_train) * 0.8)\n",
    "kitti_train_view = kitti_train.shuffle()[:index]\n",
    "kitti_validation_view = kitti_train.shuffle()[index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all images are used and none is duplicated (due to rounding of index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kitti_train_view) + len(kitti_validation_view) == len(kitti_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A dataset with name 'kitti_train_6000' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\dataset_handler.ipynb Komórka 21\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/_moje/AGH/Magisterka/Master-Thesis/model_training/OD/data/dataset_handler.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m kitti_train_6000 \u001b[39m=\u001b[39m fo\u001b[39m.\u001b[39mDataset()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/_moje/AGH/Magisterka/Master-Thesis/model_training/OD/data/dataset_handler.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m kitti_train_6000\u001b[39m.\u001b[39mmerge_samples(kitti_train_view)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/_moje/AGH/Magisterka/Master-Thesis/model_training/OD/data/dataset_handler.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m kitti_train_6000\u001b[39m.\u001b[39;49mname \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mkitti_train_6000\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\fiftyone\\core\\dataset.py:577\u001b[0m, in \u001b[0;36mDataset.name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m list_datasets():\n\u001b[1;32m--> 577\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mA dataset with name \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name)\n\u001b[0;32m    579\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_doc\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m name\n",
      "\u001b[1;31mValueError\u001b[0m: A dataset with name 'kitti_train_6000' already exists"
     ]
    }
   ],
   "source": [
    "kitti_train_6000 = fo.Dataset()\n",
    "kitti_train_6000.merge_samples(kitti_train_view)\n",
    "kitti_train_6000.name = 'kitti_train_6000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_validation_6000 = fo.Dataset()\n",
    "kitti_validation_6000.merge_samples(kitti_validation_view)\n",
    "kitti_validation_6000.name = 'kitti_validation_6000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'test' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\kitti\\test'\n",
      "Parsing dataset metadata\n",
      "Found 7518 samples\n",
      "Dataset info written to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\kitti\\info.json'\n",
      "Loading 'kitti' split 'test'\n",
      " 100% |███████████████| 7518/7518 [4.4m elapsed, 0s remaining, 27.7 samples/s]      \n",
      "Dataset 'kitti-test' created\n"
     ]
    }
   ],
   "source": [
    "kitti_test = foz.load_zoo_dataset(\"kitti\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only 1200 samples from test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_test_view = kitti_test.view()\n",
    "kitti_test_view = kitti_test_view.shuffle()[:1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_test_6000 = fo.Dataset()\n",
    "kitti_test_6000.merge_samples(kitti_test_view)\n",
    "kitti_test_6000.name = 'kitti_test_6000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_coco(split, max_samples):\n",
    "    # Restarting download after connection timeout\n",
    "    try:\n",
    "        coco_train = fo.zoo.load_zoo_dataset(\"coco-2017\", split=split, label_types=[\"detections\"], classes=[\"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"train\", \"truck\", \n",
    "        \"traffic light\", \"fire hydrant\", \"parking meter\", \"bench\", \"cat\", \"dog\", \"chair\", \"couch\", \"bed\", \"dining table\", \"tv\", \"laptop\"], only_matching=True, max_samples=max_samples)\n",
    "    except:\n",
    "        coco_train = download_coco(split, max_samples)\n",
    "\n",
    "    return coco_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\train' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_train2017.json'\n",
      "11798 images found; downloading the remaining 202\n",
      " 100% |██████████████████| 202/202 [22.0s elapsed, 0s remaining, 10.0 images/s]     \n",
      "Writing annotations for 12000 downloaded samples to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\train\\labels.json'\n",
      "Dataset info written to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\info.json'\n",
      "Loading 'coco-2017' split 'train'\n",
      " 100% |█████████████| 12000/12000 [44.4s elapsed, 0s remaining, 228.1 samples/s]      \n",
      "Dataset 'coco-2017-train-12000' created\n"
     ]
    }
   ],
   "source": [
    "coco_train = download_coco('train', 12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "300 images found; downloading the remaining 2100\n",
      "  43% |██████\\---------|  907/2100 [2.0m elapsed, 2.4m remaining, 9.2 images/s]     \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "1207 images found; downloading the remaining 1193\n",
      "   0% ||---------------|    0/1193 [21.1s elapsed, ? remaining, ? images/s] \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "1207 images found; downloading the remaining 1193\n",
      "   0% ||---------------|    0/1193 [21.2s elapsed, ? remaining, ? images/s] \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "1207 images found; downloading the remaining 1193\n",
      "   0% |----------------|    2/1193 [21.1s elapsed, 3.5h remaining, 0.1 images/s] \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "1211 images found; downloading the remaining 1189\n",
      "   0% |----------------|    2/1189 [21.1s elapsed, 3.5h remaining, 0.1 images/s] \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "1214 images found; downloading the remaining 1186\n",
      "   0% ||---------------|    4/1186 [21.1s elapsed, 1.7h remaining, 0.2 images/s] \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "1219 images found; downloading the remaining 1181\n",
      "  78% |████████████\\---|  921/1181 [2.2m elapsed, 4.3m remaining, 0.8 images/s]   \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "2140 images found; downloading the remaining 260\n",
      "   2% |\\-----------------|   5/260 [21.1s elapsed, 17.9m remaining, 0.2 images/s] \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "2146 images found; downloading the remaining 254\n",
      "   0% ||-----------------|   0/254 [21.2s elapsed, ? remaining, ? images/s] \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "2146 images found; downloading the remaining 254\n",
      "   0% |/-----------------|   1/254 [21.1s elapsed, 1.5h remaining, 0.1 images/s] \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "2148 images found; downloading the remaining 252\n",
      "   0% ||-----------------|   0/252 [21.1s elapsed, ? remaining, ? images/s] \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "2150 images found; downloading the remaining 250\n",
      "   1% |------------------|   2/250 [21.2s elapsed, 43.7m remaining, 0.1 images/s] \n",
      "Downloading split 'validation' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\instances_val2017.json'\n",
      "2153 images found; downloading the remaining 247\n",
      " 100% |██████████████████| 247/247 [34.2s elapsed, 0s remaining, 9.4 images/s]      \n",
      "Writing annotations for 2400 downloaded samples to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\validation\\labels.json'\n",
      "Dataset info written to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\info.json'\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |███████████████| 2400/2400 [20.6s elapsed, 0s remaining, 117.2 samples/s]      \n",
      "Dataset 'coco-2017-validation-2400' created\n"
     ]
    }
   ],
   "source": [
    "coco_validation = download_coco('validation', 2400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'test' to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\test' if necessary\n",
      "Test split is unlabeled; ignoring classes requirement\n",
      "Found test info at 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\coco-2017\\raw\\image_info_test2017.json'\n",
      "Sufficient images already downloaded\n",
      "Existing download of split 'test' is sufficient\n",
      "Loading existing dataset 'coco-2017-test-2400'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "coco_test = download_coco('test', 2400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BDD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing split 'train' in 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\bdd100k\\train'\n",
      "Preparing training images...\n",
      "Preparing training labels...\n",
      "Preparing validation images...\n",
      "Preparing validation labels...\n",
      "Preparing test images...\n",
      "Parsing dataset metadata\n",
      "Found 70000 samples\n",
      "Dataset info written to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\bdd100k\\info.json'\n",
      "Loading 'bdd100k' split 'train'\n",
      " 100% |█████████████| 70000/70000 [1.0h elapsed, 0s remaining, 42.3 samples/s]      \n",
      "Dataset 'bdd100k-train' created\n"
     ]
    }
   ],
   "source": [
    "bdd_train = foz.load_zoo_dataset(\n",
    "    \"bdd100k\",\n",
    "    split=\"train\",\n",
    "    source_dir=r'raw_datasets/bdd100k/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only 6000 images from shuffled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_train_view = bdd_train.view()\n",
    "bdd_train_view = bdd_train_view.shuffle()[:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save view as a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_train_6000 = fo.Dataset()\n",
    "bdd_train_6000.merge_samples(bdd_train_view)\n",
    "bdd_train_6000.name = 'bdd_train_6000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'validation' already prepared\n",
      "Loading existing dataset 'bdd100k-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "bdd_validation = foz.load_zoo_dataset(\n",
    "    \"bdd100k\",\n",
    "    split=\"validation\",\n",
    "    source_dir=r'raw_datasets/bdd100k/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only 1200 images from shuffled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_validation_view = bdd_validation.view()\n",
    "bdd_validation_view = bdd_validation_view.shuffle()[:1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save view as a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_validation_6000 = fo.Dataset()\n",
    "bdd_validation_6000.merge_samples(bdd_validation_view)\n",
    "bdd_validation_6000.name = 'bdd_validation_6000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing split 'test' in 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\bdd100k\\test'\n",
      "Parsing dataset metadata\n",
      "Found 20000 samples\n",
      "Dataset info written to 'D:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\raw_datasets\\bdd100k\\info.json'\n",
      "Loading 'bdd100k' split 'test'\n",
      " 100% |█████████████| 20000/20000 [3.6m elapsed, 0s remaining, 102.2 samples/s]      \n",
      "Dataset 'bdd100k-test' created\n"
     ]
    }
   ],
   "source": [
    "bdd_test = foz.load_zoo_dataset(\n",
    "    \"bdd100k\",\n",
    "    split=\"test\",\n",
    "    source_dir=r'raw_datasets/bdd100k/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only 1200 images from shuffled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_test_view = bdd_test.view()\n",
    "bdd_test_view = bdd_test_view.shuffle()[:1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save view as new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_test_6000 = fo.Dataset()\n",
    "bdd_test_6000.merge_samples(bdd_test_view)\n",
    "bdd_test_6000.name = 'bdd_test_6000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to run this if \"Raw dataset preparation\" was run in this session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_train_6000 = fo.load_dataset('kitti_train_6000')\n",
    "bdd_train_6000 = fo.load_dataset('bdd_train_6000')\n",
    "coco_train = fo.load_dataset('coco-2017-train-12000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_validation_6000 = fo.load_dataset('kitti_validation_6000')\n",
    "bdd_validation_6000 = fo.load_dataset('bdd_validation_6000')\n",
    "coco_validation = fo.load_dataset('coco-2017-validation-2400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_test_6000 = fo.load_dataset('kitti_test_6000')\n",
    "bdd_test_6000 = fo.load_dataset('bdd_test_6000')\n",
    "coco_test = fo.load_dataset('coco-2017-test-2400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Record Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to preproces data only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete images without labels and containing only \"traffic sign\" label from BDD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bdd(ds):\n",
    "    dropped_ids = []\n",
    "    for img in ds:\n",
    "        if img.detections:\n",
    "            det_labels = []\n",
    "            for det in img.detections.detections:\n",
    "                det_labels.append(det.label)\n",
    "            det_labels = set(det_labels)\n",
    "            if len(det_labels) == 1 and \"traffic sign\" in det_labels:\n",
    "                dropped_ids.append(img.id)\n",
    "        else:\n",
    "            dropped_ids.append(img.id)\n",
    "\n",
    "    ds.delete_samples(dropped_ids)\n",
    "    ds.save()\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_train_6000 = preprocess_bdd(bdd_train_6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_validation_6000 = preprocess_bdd(bdd_validation_6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete images without labels and with labels containig only \"Misc\" or \"DontCare\" from KITTI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_kitti(ds):\n",
    "    correct_labels = [\"Car\", \"Pedestrian\", \"Van\", \"Cyclist\", \"Truck\", \"Tram\", \"Person_sitting\"]\n",
    "\n",
    "    dropped_ids = []\n",
    "    for img in ds:\n",
    "        det_labels = []\n",
    "        for det in img.ground_truth.detections:\n",
    "            det_labels.append(det.label)\n",
    "        for label in correct_labels:\n",
    "            if label in det_labels:\n",
    "                break\n",
    "            dropped_ids.append(img.id)\n",
    "\n",
    "    ds.delete_samples(dropped_ids)\n",
    "    ds.save()\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_train_6000 = preprocess_kitti(kitti_train_6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_validation_6000 = preprocess_kitti(kitti_validation_6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input datasets statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetStats:\n",
    "    def __init__(self, bdd, kitti, coco):\n",
    "        self.bdd_dataset = bdd\n",
    "        self.kitti_dataset = kitti\n",
    "        self.coco_dataset = coco\n",
    "\n",
    "        coco_labels = self.get_gt_labels(coco)\n",
    "        bdd_labels = self.get_detection_labels(bdd)\n",
    "        kitti_labels = self.get_gt_labels(kitti)\n",
    "\n",
    "        self.bdd = pd.DataFrame({\"labels\": bdd_labels})\n",
    "        self.coco = pd.DataFrame({\"labels\": coco_labels})\n",
    "        self.kitti = pd.DataFrame({\"labels\": kitti_labels})\n",
    "\n",
    "    def get_detection_labels(self, ds):\n",
    "        labels = []\n",
    "        for img in ds:\n",
    "            for det in img.detections.detections:\n",
    "                labels.append(det.label)\n",
    "        \n",
    "        return labels\n",
    "\n",
    "    def get_gt_labels(self, ds):\n",
    "        labels = []\n",
    "        for img in ds:\n",
    "            for det in img.ground_truth.detections:\n",
    "                labels.append(det.label)\n",
    "\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bdd_train_6000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\_moje\\AGH\\Magisterka\\Master-Thesis\\model_training\\OD\\data\\dataset_handler.ipynb Komórka 68\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/_moje/AGH/Magisterka/Master-Thesis/model_training/OD/data/dataset_handler.ipynb#Y245sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_stats \u001b[39m=\u001b[39m DatasetStats(bdd_train_6000, kitti_train_6000, coco_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bdd_train_6000' is not defined"
     ]
    }
   ],
   "source": [
    "train_stats = DatasetStats(bdd_train_6000, kitti_train_6000, coco_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.coco.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.kitti.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats.bdd.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_stats = DatasetStats(bdd_validation_6000, kitti_validation_6000, coco_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels       \n",
       "person           7123\n",
       "car              1217\n",
       "chair            1084\n",
       "dining table      429\n",
       "traffic light     414\n",
       "bench             281\n",
       "truck             258\n",
       "motorcycle        245\n",
       "bicycle           224\n",
       "bus               196\n",
       "tv                173\n",
       "couch             148\n",
       "laptop            142\n",
       "dog               141\n",
       "cat               127\n",
       "train             111\n",
       "bed                98\n",
       "fire hydrant       70\n",
       "parking meter      35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_stats.coco.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels        \n",
       "Car               5719\n",
       "DontCare          2046\n",
       "Van                528\n",
       "Pedestrian         426\n",
       "Cyclist            278\n",
       "Truck              196\n",
       "Misc               179\n",
       "Tram                73\n",
       "Person_sitting      19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_stats.kitti.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels       \n",
       "car              12265\n",
       "traffic sign      4105\n",
       "traffic light     3066\n",
       "person            1743\n",
       "truck              499\n",
       "bus                174\n",
       "bike               141\n",
       "rider               98\n",
       "motor               42\n",
       "train                1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_stats.bdd.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord blueprint as DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map datasets default labels to expected labels:\n",
    "\n",
    "        1 - person\n",
    "        2 - car\n",
    "        3 - big_car\n",
    "        4 - bike\n",
    "        5 - train\n",
    "        6 - traffic_light\n",
    "        7 - animal\n",
    "        8 - obstacle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFHandler:\n",
    "    def prepare_input(self, data, ids, filenames, bounding_boxes, labels):\n",
    "        for bbox, label in zip(data['bboxes'], data['labels']):\n",
    "            ids.append(data[\"id\"])\n",
    "            filenames.append(data['filename'])\n",
    "            bounding_boxes.append(bbox)\n",
    "            labels.append(label)\n",
    "\n",
    "        return ids, filenames, bounding_boxes, labels\n",
    "\n",
    "    def handle(self, dataset, ids, filenames, bounding_boxes, labels):\n",
    "        for img in dataset:\n",
    "            data = self.preprocess_data(img)\n",
    "            ids, filenames, bounding_boxes, labels = self.prepare_input(data, ids, filenames, bounding_boxes, labels)\n",
    "\n",
    "        return ids, filenames, bounding_boxes, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOHandler(DFHandler):\n",
    "    label_map = {\n",
    "        \"person\": 1,\n",
    "        \"car\": 2,\n",
    "        \"chair\": 8,\n",
    "        \"dining table\": 8,\n",
    "        \"traffic light\": 6,\n",
    "        \"truck\": 3,\n",
    "        \"bench\": 8,\n",
    "        \"motorcycle\": 4,\n",
    "        \"bicycle\": 4,\n",
    "        \"dog\": 7,\n",
    "        \"bus\": 3,\n",
    "        \"couch\": 8,\n",
    "        \"tv\": 8,\n",
    "        \"laptop\": 8,\n",
    "        \"train\": 5,\n",
    "        \"cat\": 7,\n",
    "        \"bed\": 8,\n",
    "        \"fire hydrant\": 8,\n",
    "        \"parking meter\": 8,\n",
    "    }\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        labels = []\n",
    "        bboxes = []\n",
    "        for detection in data.ground_truth.detections:\n",
    "            bboxes.append(detection.bounding_box)\n",
    "            labels.append(self.label_map[detection.label])\n",
    "\n",
    "        return {\"id\": f\"{data['id']}_coco\", 'filename': data[\"filepath\"], \"labels\": labels, \"bboxes\":bboxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDDHandler(DFHandler):\n",
    "    label_map = {\n",
    "        'traffic light': 6,\n",
    "        'car': 2,\n",
    "        'person': 1,\n",
    "        'motor': 4,\n",
    "        'bus': 3,\n",
    "        'truck': 3,\n",
    "        'bike': 4,\n",
    "        'train': 5,\n",
    "    }\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        labels = []\n",
    "        bboxes = []\n",
    "        for detection in data.detections.detections:\n",
    "            if detection.label in self.label_map:\n",
    "                bboxes.append(detection.bounding_box)\n",
    "                labels.append(self.label_map[detection.label])\n",
    "\n",
    "        return {\"id\": f\"{data['id']}_bdd\", 'filename': data[\"filepath\"], \"labels\": labels, \"bboxes\":bboxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIHandler(DFHandler):\n",
    "    label_map = {\n",
    "        'Car': 2,\n",
    "        'Van': 3,\n",
    "        'Pedestrian': 1,\n",
    "        'Truck': 3,\n",
    "        'Cyclist': 4,\n",
    "        'Tram': 5,\n",
    "        'Person_sitting': 1\n",
    "    }\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        labels = []\n",
    "        bboxes = []\n",
    "        for detection in data.ground_truth.detections:\n",
    "            if detection.label in self.label_map:\n",
    "                bboxes.append(detection.bounding_box)\n",
    "                labels.append(self.label_map[detection.label])\n",
    "\n",
    "        return {\"id\": f\"{data['id']}_kitti\", 'filename': data[\"filepath\"], \"labels\": labels, \"bboxes\":bboxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pandas_blueprint(coco_ds, bdd_ds, kitti_ds):\n",
    "    coco_handler = COCOHandler()\n",
    "    bdd_handler = BDDHandler()\n",
    "    kitti_handler = KITTIHandler()\n",
    "\n",
    "    id, filename, boxes, labels = coco_handler.handle(coco_ds, [], [], [], [])\n",
    "    id, filename, boxes, labels = bdd_handler.handle(bdd_ds, id, filename, boxes, labels)\n",
    "    id, filename, boxes, labels = kitti_handler.handle(kitti_ds, id, filename, boxes, labels)\n",
    "\n",
    "    return pd.DataFrame({\"id\": id,\"filename\": filename, \"bboxes\": boxes, \"label\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_pandas_blueprint(coco_train, bdd_train_6000, kitti_train_6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = create_pandas_blueprint(coco_validation, bdd_validation_6000, kitti_validation_6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    label_map = {\n",
    "        1: \"person\".encode('utf8'),\n",
    "        2: \"car\".encode('utf8'),\n",
    "        3: \"big_car\".encode('utf8'),\n",
    "        4: \"bike\".encode('utf8'),\n",
    "        5: \"train\".encode('utf8'),\n",
    "        6: \"traffic_light\".encode('utf8'),\n",
    "        7: \"animal\".encode('utf8'),\n",
    "        8: \"obstacle\".encode('utf8')\n",
    "    }\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.filename = data['filename'].encode('utf8')\n",
    "        self.source_id = self.filename\n",
    "\n",
    "        self.encoded_image = self.get_encoded_image(data)\n",
    "        \n",
    "        height, width, format = self.get_image_data(data)\n",
    "        self.height, self.width, self.format = height, width, format\n",
    "        \n",
    "        self.label = []\n",
    "        self.text = []\n",
    "        self.xmin, self.xmax, self.ymin, self.ymax = [], [], [], []\n",
    "\n",
    "    def get_encoded_image(self, data):\n",
    "        with tf.io.gfile.GFile(data['filename'], 'rb') as fid:\n",
    "            encoded_jpg = fid.read()\n",
    "\n",
    "            if encoded_jpg == b'': raise IOError('Corrupted file')\n",
    "        return encoded_jpg\n",
    "\n",
    "    def get_image_data(self, data):\n",
    "        encoded_jpg_io = io.BytesIO(self.encoded_image)\n",
    "        image = Image.open(encoded_jpg_io)\n",
    "\n",
    "        # Check for corruption in jpg files\n",
    "        try:\n",
    "            image.getdata()[0]\n",
    "        except:\n",
    "            raise IOError(\"Corrupted file\")\n",
    "\n",
    "        width, height = image.size\n",
    "\n",
    "        image_format = data['filename'][-3:].encode('utf8')\n",
    "\n",
    "        return height, width, image_format\n",
    "\n",
    "    def add_bounding_box(self, data):\n",
    "        self.xmin.append(data['bboxes'][0])\n",
    "        self.xmax.append(data['bboxes'][0] + data['bboxes'][2])\n",
    "        self.ymin.append(data['bboxes'][1])\n",
    "        self.ymax.append(data['bboxes'][1] + data['bboxes'][3])\n",
    "\n",
    "    def add_label(self, data):\n",
    "        self.label.append(data['label'])\n",
    "        self.text.append(self.label_map[data['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRecordWriter:\n",
    "  \n",
    "    def __init__(self, dataset_df, path, type):\n",
    "        self.df = dataset_df\n",
    "        self.type = type\n",
    "        self.path = path\n",
    "\n",
    "    def write_record(self):\n",
    "        ids = self.prepare_index()\n",
    "        self.open_file()\n",
    "\n",
    "        for id in ids:\n",
    "            data = self.df.query(f'id == \"{id}\"')\n",
    "            try:\n",
    "                sample = Sample(data.iloc[0])\n",
    "\n",
    "                for index, row in data.iterrows():\n",
    "                    sample.add_bounding_box(row)\n",
    "                    sample.add_label(row)\n",
    "\n",
    "                tf_example = self.create_tf_example(sample)\n",
    "                self.write_to_file(tf_example) \n",
    "\n",
    "            except IOError:\n",
    "                pass\n",
    "            \n",
    "        self.close_file\n",
    "\n",
    "    def prepare_index(self):\n",
    "        ids = self.df[\"id\"].unique()\n",
    "        shuffle(ids)\n",
    "        return ids\n",
    "\n",
    "    def open_file(self):\n",
    "        self.writer = tf.io.TFRecordWriter(rf\"{self.path}/{self.type}.records\")\n",
    "\n",
    "    def write_to_file(self, tf_example):\n",
    "        self.writer.write(tf_example.SerializeToString()) \n",
    "\n",
    "    def close_file(self):\n",
    "        self.writer.close()\n",
    "\n",
    "    def create_tf_example(self, sample):\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'image/height': dataset_util.int64_feature(sample.height),\n",
    "            'image/width': dataset_util.int64_feature(sample.width),\n",
    "            'image/filename': dataset_util.bytes_feature(sample.filename),\n",
    "            'image/source_id': dataset_util.bytes_feature(sample.source_id),\n",
    "            'image/encoded': dataset_util.bytes_feature(sample.encoded_image),\n",
    "            'image/format': dataset_util.bytes_feature(sample.format),\n",
    "            'image/object/bbox/xmin': dataset_util.float_list_feature(sample.xmin),\n",
    "            'image/object/bbox/xmax': dataset_util.float_list_feature(sample.xmax),\n",
    "            'image/object/bbox/ymin': dataset_util.float_list_feature(sample.ymin),\n",
    "            'image/object/bbox/ymax': dataset_util.float_list_feature(sample.ymax),\n",
    "            'image/object/class/text': dataset_util.bytes_list_feature(sample.text),\n",
    "            'image/object/class/label': dataset_util.int64_list_feature(sample.label),\n",
    "        }))\n",
    "        return tf_example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record_writer = TFRecordWriter(train_df, r\"prepared_datasets\", \"train\")\n",
    "train_record_writer.write_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_record_writer = TFRecordWriter(validation_df, r\"prepared_datasets\", \"validation\")\n",
    "validation_record_writer.write_record()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('master_thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa476f7de59cb8a012dad243296bb920de54536d7a00d5cb171234a4719cdc64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
